select(date, text, retweets)
knitr::opts_chunk$set(echo = TRUE)
# Suppress startup messages from packages
suppressPackageStartupMessages(library(tidyverse))
# Load necessary libraries
library(readr)      # For reading data
library(dplyr)      # For data manipulation
library(ggplot2)    # For data visualization
library(tidyverse)  # Comprehensive data manipulation and visualization package
library(glmnet)     # For fitting Lasso and Elastic-Net regularized generalized linear models
library(stringr)    # For string manipulation
library(tokenizers) # For tokenization
library(lubridate)  # For working with dates
library(tidytext)   # For text mining with tidy data principles
# Check if tidyverse package is installed, if not, install it
if (!requireNamespace("tidyverse", quietly = TRUE)) {
install.packages("tidyverse")
}
# Define file directory and path
trump_data <- read_csv(file.path("./", "twitter", "realDonaldTrump-20201106.csv"), col_types=cols(id=col_character()))
# Load stop words
data(stop_words)
# Define additional stop words
extra <- c("realdonaldtrump", "donaldtrump", "donald", "trump", "rt") # Define additional words to be considered as stop words
# Process tweets
tidy <- trump_data %>% # Start processing data using piping
filter(!isRetweet, str_detect(text, "[:space:]")) %>% # Filter out retweets and tweets without spaces
select(id, text, retweets, date) %>% # Select necessary columns
unnest_tokens("word", text, token = "regex") %>% # Tokenize the text column into individual words
mutate( # Replace words starting with @ with AT
word=str_replace_all(word, "[[:punct:]&&[^#]]", ""), # Remove punctuation from words
) %>% # Replace AT with @
anti_join(stop_words, by = "word", copy = TRUE) %>% # Remove stop words
filter(
!word %in% extra, # Filter out additional stop words
!str_detect(word, "@"), # Filter out words containing @
!str_detect(word, "amp"), # Filter out words containing "amp"
!str_detect(word, "http"), # Filter out words containing "http"
str_length(word) > 0L # Filter out empty words
)
# Visualize top 20 most common terms
top_20_terms <- tidy %>% # Start processing tidy dataframe
count(word) %>% # Count the frequency of each word
top_n(20) # Select top 20 words
# Plot
ggplot(top_20_terms, aes(x = reorder(word, n), y = n)) + # Create ggplot with word frequency data
geom_col(fill = "green") + # Add column plot with light blue color
coord_flip() + # Flip the coordinates
labs(x = "Term", y = "Count", title = "Top terms in Donald Trump's tweets") + # Add labels to axes and title
theme_minimal() # Apply minimal theme
knitr::opts_chunk$set(echo = TRUE)
# Suppress startup messages from packages
suppressPackageStartupMessages(library(tidyverse))
# Load necessary libraries
library(readr)      # For reading data
library(dplyr)      # For data manipulation
library(ggplot2)    # For data visualization
library(tidyverse)  # Comprehensive data manipulation and visualization package
library(glmnet)     # For fitting Lasso and Elastic-Net regularized generalized linear models
library(stringr)    # For string manipulation
library(tokenizers) # For tokenization
library(lubridate)  # For working with dates
library(tidytext)   # For text mining with tidy data principles
# Check if tidyverse package is installed, if not, install it
if (!requireNamespace("tidyverse", quietly = TRUE)) {
install.packages("tidyverse")
}
# Define file directory and path
trump_data <- read_csv(file.path("./", "twitter", "realDonaldTrump-20201106.csv"), col_types=cols(id=col_character()))
# Load stop words
data(stop_words)
# Define additional stop words
extra <- c("realdonaldtrump", "donaldtrump", "donald", "trump", "rt") # Define additional words to be considered as stop words
# Process tweets
tidy <- trump_data %>% # Start processing data using piping
filter(!isRetweet, str_detect(text, "[:space:]")) %>% # Filter out retweets and tweets without spaces
select(id, text, retweets, date) %>% # Select necessary columns
unnest_tokens("word", text, token = "regex") %>% # Tokenize the text column into individual words
mutate( # Replace words starting with @ with AT
word=str_replace_all(word, "[[:punct:]&&[^#]]", ""), # Remove punctuation from words
) %>% # Replace AT with @
anti_join(stop_words, by = "word", copy = TRUE) %>% # Remove stop words
filter(
!word %in% extra, # Filter out additional stop words
!str_detect(word, "@"), # Filter out words containing @
!str_detect(word, "amp"), # Filter out words containing "amp"
!str_detect(word, "http"), # Filter out words containing "http"
str_length(word) > 0L # Filter out empty words
)
# Visualize top 20 most common terms
top_20_terms <- tidy %>% # Start processing tidy dataframe
count(word) %>% # Count the frequency of each word
top_n(20) # Select top 20 words
# Plot
ggplot(top_20_terms, aes(x = reorder(word, n), y = n)) + # Create ggplot with word frequency data
geom_col(fill = "green") + # Add column plot with light blue color
coord_flip() + # Flip the coordinates
labs(x = "Term", y = "Count", title = "Top terms in Donald Trump's tweets") + # Add labels to axes and title
theme_minimal() # Apply minimal theme
# Load necessary libraries
library(lubridate)  # For handling date data
library(ggplot2)    # For data visualization
library(viridis)    # For color scales
# Top 20 terms for each year
top_20_terms <- tidy %>%
mutate(year = as.integer(format(date, "%Y"))) %>% # Extract year from date
filter(year >= 2015 & year <= 2020) %>% # Filter data for years 2015 to 2020
count(word, year) %>% # Count occurrences of each word by year
group_by(year) %>% # Group by year
top_n(20) %>% # Select top 20 terms for each year
ungroup()
# Identify terms that start with a hashtag
hashtag_terms <- top_20_terms %>%
filter(stringr::str_detect(word, "^#")) # Filter terms starting with a hashtag
# Create a column for labels with hashtags for terms starting with a hashtag
top_20_terms$label <- top_20_terms$word
top_20_terms$label[hashtag_terms$word] <- paste0("#", hashtag_terms$word)
knitr::opts_chunk$set(echo = TRUE)
# Suppress startup messages from packages
suppressPackageStartupMessages(library(tidyverse))
# Load necessary libraries
library(readr)      # For reading data
library(dplyr)      # For data manipulation
library(ggplot2)    # For data visualization
library(tidyverse)  # Comprehensive data manipulation and visualization package
library(glmnet)     # For fitting Lasso and Elastic-Net regularized generalized linear models
library(stringr)    # For string manipulation
library(tokenizers) # For tokenization
library(lubridate)  # For working with dates
library(tidytext)   # For text mining with tidy data principles
# Check if tidyverse package is installed, if not, install it
if (!requireNamespace("tidyverse", quietly = TRUE)) {
install.packages("tidyverse")
}
# Define file directory and path
trump_data <- read_csv(file.path("./", "twitter", "realDonaldTrump-20201106.csv"), col_types=cols(id=col_character()))
# Load stop words
data(stop_words)
# Define additional stop words
extra <- c("realdonaldtrump", "donaldtrump", "donald", "trump", "rt") # Define additional words to be considered as stop words
# Process tweets
tidy <- trump_data %>% # Start processing data using piping
filter(!isRetweet, str_detect(text, "[:space:]")) %>% # Filter out retweets and tweets without spaces
select(id, text, retweets, date) %>% # Select necessary columns
unnest_tokens("word", text, token = "regex") %>% # Tokenize the text column into individual words
mutate( # Replace words starting with @ with AT
word=str_replace_all(word, "[[:punct:]&&[^#]]", ""), # Remove punctuation from words
) %>% # Replace AT with @
anti_join(stop_words, by = "word", copy = TRUE) %>% # Remove stop words
filter(
!word %in% extra, # Filter out additional stop words
!str_detect(word, "@"), # Filter out words containing @
!str_detect(word, "amp"), # Filter out words containing "amp"
!str_detect(word, "http"), # Filter out words containing "http"
str_length(word) > 0L # Filter out empty words
)
# Visualize top 20 most common terms
top_20_terms <- tidy %>% # Start processing tidy dataframe
count(word) %>% # Count the frequency of each word
top_n(20) # Select top 20 words
# Plot
ggplot(top_20_terms, aes(x = reorder(word, n), y = n)) + # Create ggplot with word frequency data
geom_col(fill = "green") + # Add column plot with light blue color
coord_flip() + # Flip the coordinates
labs(x = "Term", y = "Count", title = "Top terms in Donald Trump's tweets") + # Add labels to axes and title
theme_minimal() # Apply minimal theme
# Load necessary libraries
library(lubridate)  # For handling date data
library(ggplot2)    # For data visualization
library(viridis)    # For color scales
# Top 20 terms for each year
top_20_terms <- tidy %>%
mutate(year = as.integer(format(date, "%Y"))) %>% # Extract year from date
filter(year >= 2015 & year <= 2020) %>% # Filter data for years 2015 to 2020
count(word, year) %>% # Count occurrences of each word by year
group_by(year) %>% # Group by year
top_n(20) %>% # Select top 20 terms for each year
ungroup()
# Identify terms that start with a hashtag
hashtag_terms <- top_20_terms %>%
filter(stringr::str_detect(word, "^#")) # Filter terms starting with a hashtag
# Create a column for labels with hashtags for terms starting with a hashtag
top_20_terms$label <- top_20_terms$word
#top_20_terms$label[hashtag_terms$word] <- paste0("#", hashtag_terms$word)
# Plotting top 10 terms
ggplot(top_20_terms, aes(x = reorder_within(word, n, year), y = n, fill = factor(year))) + # Start plotting
geom_col(color = "white", size = 0.25) + # Add column plot with white outline
coord_flip() + # Flip the coordinates
scale_x_reordered() + # Reorder x-axis labels
scale_y_continuous(labels = NULL) +   # Remove y-axis labels
scale_fill_viridis(discrete = TRUE, option = "magma") + # Set color scale
facet_wrap(~year, scales = "free", ncol = 2) + # Wrap facets by year, adjust scales, and arrange in two columns
labs(x = "Term", y = "Count", title = "Top 20 Common Terms in Trump's Tweets by Year") + # Add labels and title
theme_minimal() + # Apply minimal theme
theme( # Customize the plot appearance
text = element_text(size = 6), # Set text size
axis.title = element_text(face = "bold", size = 20), # Set axis title appearance
panel.spacing = unit(10, "pt"), # Set panel spacing
plot.title = element_text(face = "bold", size = 14), # Set plot title appearance
panel.background = element_rect(fill = "lightblue") # Set light blue background
)
# Load necessary libraries
library(tidytext)   # For text mining with tidy data principles
library(dplyr)      # For data manipulation
library(ggplot2)    # For data visualization
# Assuming trump_tidy is already processed and in the correct format
# Create year variable from date
tidy <- tidy %>%
mutate(year = lubridate::year(date)) # Add a new column "year" by extracting the year from the "date" column
# Calculate TF-IDF for each term and year
trump_year_wise_tweets <- tidy %>%
filter(year >= 2015 & year <= 2020) %>% # Filter data for the years 2015 to 2020
count(word, year) %>%                   # Count the occurrences of each word by year
bind_tf_idf(word, year, n) %>%          # Calculate TF-IDF for each term and year
arrange(desc(tf_idf))                   # Arrange the data in descending order of TF-IDF score
# Visualize the top 20 characteristic terms for each year
top_terms_by_year <- trump_year_wise_tweets %>%
group_by(year) %>%                       # Group the data by year
top_n(20, wt = tf_idf)                   # Select the top 20 terms with the highest TF-IDF score for each year
# Plotting
ggplot(top_terms_by_year, aes(x = reorder_within(word, tf_idf, year), y = tf_idf, fill = factor(year))) + # Start plotting
geom_col(show.legend = FALSE) +          # Add column plot without legend
coord_flip() +                           # Flip the coordinates
scale_x_reordered() +                    # Reorder x-axis labels
scale_y_continuous(labels = NULL) +      # Remove y-axis labels
scale_fill_viridis(discrete = TRUE, option = "magma") +  # Set color scale to light colors using Viridis
facet_wrap(~year, scales = "free", ncol = 2) +           # Wrap facets by year, adjust scales, and arrange in two columns
labs(x = "Term", y = "tf-idf",           # Add labels to axes
title = "Top 20 Characteristic Terms from Trump's Tweets for Each Year") + # Add title
theme(text = element_text(size = 6),      # Set text size
axis.title = element_text(face = "bold", size = 20),panel.spacing = unit(10, "pt"),  # Customize axis title appearance and panel spacing
plot.title = element_text(face = "bold", size = 12)  )  # Customize plot title appearance
# Filter the data to include only tweets from 2016-2020
filtered_data <- tidy %>%
filter(year >= 2016 & year <= 2020)
# Create a document-term matrix
dtm <- filtered_data %>%
count(id, word) %>%
cast_sparse(id, word, n)
# Display dimensions of the document-term matrix
dim(dtm)
# Extract the tweet IDs to get the target variable (# of retweets)
ids <- tibble(id=rownames(dtm))
ids <- left_join(ids, trump_data)
# Define the target variable
retweets <- ids$retweets
# Set seed for reproducibility
set.seed(2020)
# Fit Lasso and Elastic-Net regularized generalized linear models
fit_cv <- cv.glmnet(dtm, retweets)
# Visualize the cross-validation results
plot(fit_cv)
# Display cross-validation results
fit_cv
# Extract the selected value of lambda
selected_lambda <- fit_cv$lambda.min
cat("Selected value of lambda:", selected_lambda, "\n")
# Get the number of non-zero coefficients
coefficients <- coef(fit_cv, s = selected_lambda)
non_zero_coefficients <- sum(coefficients != 0)
cat("Number of non-zero coefficients:", non_zero_coefficients, "\n")
# Extract coefficients from the best model
coefficients <- coef(fit_cv, s = "lambda.min")
# Convert coefficients to a tibble
coefficients <- tibble(word = rownames(coefficients), coef = as.numeric(coefficients))
# Select the top 15 coefficients
top_15_terms <- coefficients %>%
top_n(15)
# Plot the top 15 terms with their coefficients
library(ggplot2)
ggplot(top_15_terms, aes(x = reorder(word, coef), y = coef/10)) + # Create ggplot with top 15 terms and their coefficients
geom_col(fill = "orange", width = 0.5) + # Add column plot with orange color
coord_flip() + # Flip the coordinates
labs(x = "Term", y = "Regression Coefficient", # Add labels to axes
title = "Top 15 Terms with Regression Coefficients") + # Add title
theme_minimal() + # Apply minimal theme
theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Rotate x-axis labels
trump_lower <- trump_data %>%
filter(year(date) >= 2016) %>%
mutate(text=str_to_lower(text))
trump_lower %>%
filter(str_detect(text, "#fnn")) %>%
select(date, text, retweets)
trump_lower %>%
filter(str_detect(text, "insult")) %>%
select(date, text, retweets)
# Filter the dataframe to include only tweets containing the word "starved"
trump_lower %>%
filter(str_detect(text, "starved")) %>%
# Select relevant columns for analysis
select(date, text, retweets)
# Filter tweets containing the pattern "weli" using the lowercased text
trump_lower %>%
filter(str_detect(text, "weli")) %>%
# Select relevant columns for display
select(date, text, retweets)
# Filter the trump_lower dataframe to include only tweets containing the word "quarantine"
trump_lower %>%
filter(str_detect(text, "quarantine")) %>%
select(date, text, retweets)
# Filter tweets containing the term "covfefe"
trump_lower %>%
filter(str_detect(text, "covfefe")) %>%
# Select specific columns for analysis
select(date, text, retweets)
# Filter the 'trump_lower' dataset to include only rows where the text contains the pattern "way[:punct:]+and"
trump_lower %>%
filter(str_detect(text, "way[:punct:]+and")) %>%
# Select specific columns for output
select(date, text, retweets)
# Filter tweets containing the pattern "amazing[:punct:]+tremendous"
trump_lower %>%
filter(str_detect(text, "amazing[:punct:]+tremendous")) %>%
# Select columns for date, text, and retweets
select(date, text, retweets)
knitr::opts_chunk$set(echo = TRUE)
# Suppress startup messages from packages
suppressPackageStartupMessages(library(tidyverse))
# Load necessary libraries
library(readr)      # For reading data
library(dplyr)      # For data manipulation
library(ggplot2)    # For data visualization
library(tidyverse)  # Comprehensive data manipulation and visualization package
library(glmnet)     # For fitting Lasso and Elastic-Net regularized generalized linear models
library(stringr)    # For string manipulation
library(tokenizers) # For tokenization
library(lubridate)  # For working with dates
library(tidytext)   # For text mining with tidy data principles
# Check if tidyverse package is installed, if not, install it
if (!requireNamespace("tidyverse", quietly = TRUE)) {
install.packages("tidyverse")
}
# Define file directory and path
trump_data <- read_csv(file.path("./", "twitter", "realDonaldTrump-20201106.csv"), col_types=cols(id=col_character()))
# Load stop words
data(stop_words)
# Define additional stop words
extra <- c("realdonaldtrump", "donaldtrump", "donald", "trump", "rt") # Define additional words to be considered as stop words
# Process tweets
tidy <- trump_data %>% # Start processing data using piping
filter(!isRetweet, str_detect(text, "[:space:]")) %>% # Filter out retweets and tweets without spaces
select(id, text, retweets, date) %>% # Select necessary columns
unnest_tokens("word", text, token = "regex") %>% # Tokenize the text column into individual words
mutate( # Replace words starting with @ with AT
word=str_replace_all(word, "[[:punct:]&&[^#]]", ""), # Remove punctuation from words
) %>% # Replace AT with @
anti_join(stop_words, by = "word", copy = TRUE) %>% # Remove stop words
filter(
!word %in% extra, # Filter out additional stop words
!str_detect(word, "@"), # Filter out words containing @
!str_detect(word, "amp"), # Filter out words containing "amp"
!str_detect(word, "http"), # Filter out words containing "http"
str_length(word) > 0L # Filter out empty words
)
# Visualize top 20 most common terms
top_20_terms <- tidy %>% # Start processing tidy dataframe
count(word) %>% # Count the frequency of each word
top_n(20) # Select top 20 words
# Plot
ggplot(top_20_terms, aes(x = reorder(word, n), y = n)) + # Create ggplot with word frequency data
geom_col(fill = "green") + # Add column plot with light blue color
coord_flip() + # Flip the coordinates
labs(x = "Term", y = "Count", title = "Top terms in Donald Trump's tweets") + # Add labels to axes and title
theme_minimal() # Apply minimal theme
# Load necessary libraries
library(lubridate)  # For handling date data
library(ggplot2)    # For data visualization
library(viridis)    # For color scales
# Top 20 terms for each year
top_20_terms <- tidy %>%
mutate(year = as.integer(format(date, "%Y"))) %>% # Extract year from date
filter(year >= 2015 & year <= 2020) %>% # Filter data for years 2015 to 2020
count(word, year) %>% # Count occurrences of each word by year
group_by(year) %>% # Group by year
top_n(20) %>% # Select top 20 terms for each year
ungroup()
# Identify terms that start with a hashtag
hashtag_terms <- top_20_terms %>%
filter(stringr::str_detect(word, "^#")) # Filter terms starting with a hashtag
# Create a column for labels with hashtags for terms starting with a hashtag
top_20_terms$label <- top_20_terms$word
# Plotting top 10 terms
ggplot(top_20_terms, aes(x = reorder_within(word, n, year), y = n, fill = factor(year))) + # Start plotting
geom_col(color = "white", size = 0.25) + # Add column plot with white outline
coord_flip() + # Flip the coordinates
scale_x_reordered() + # Reorder x-axis labels
scale_y_continuous(labels = NULL) +   # Remove y-axis labels
scale_fill_viridis(discrete = TRUE, option = "magma") + # Set color scale
facet_wrap(~year, scales = "free", ncol = 2) + # Wrap facets by year, adjust scales, and arrange in two columns
labs(x = "Term", y = "Count", title = "Top 20 Common Terms in Trump's Tweets by Year") + # Add labels and title
theme_minimal() + # Apply minimal theme
theme( # Customize the plot appearance
text = element_text(size = 6), # Set text size
axis.title = element_text(face = "bold", size = 20), # Set axis title appearance
panel.spacing = unit(10, "pt"), # Set panel spacing
plot.title = element_text(face = "bold", size = 14), # Set plot title appearance
panel.background = element_rect(fill = "lightblue") # Set light blue background
)
# Load necessary libraries
library(tidytext)   # For text mining with tidy data principles
library(dplyr)      # For data manipulation
library(ggplot2)    # For data visualization
# Assuming trump_tidy is already processed and in the correct format
# Create year variable from date
tidy <- tidy %>%
mutate(year = lubridate::year(date)) # Add a new column "year" by extracting the year from the "date" column
# Calculate TF-IDF for each term and year
trump_year_wise_tweets <- tidy %>%
filter(year >= 2015 & year <= 2020) %>% # Filter data for the years 2015 to 2020
count(word, year) %>%                   # Count the occurrences of each word by year
bind_tf_idf(word, year, n) %>%          # Calculate TF-IDF for each term and year
arrange(desc(tf_idf))                   # Arrange the data in descending order of TF-IDF score
# Visualize the top 20 characteristic terms for each year
top_terms_by_year <- trump_year_wise_tweets %>%
group_by(year) %>%                       # Group the data by year
top_n(20, wt = tf_idf)                   # Select the top 20 terms with the highest TF-IDF score for each year
# Plotting
ggplot(top_terms_by_year, aes(x = reorder_within(word, tf_idf, year), y = tf_idf, fill = factor(year))) + # Start plotting
geom_col(show.legend = FALSE) +          # Add column plot without legend
coord_flip() +                           # Flip the coordinates
scale_x_reordered() +                    # Reorder x-axis labels
scale_y_continuous(labels = NULL) +      # Remove y-axis labels
scale_fill_viridis(discrete = TRUE, option = "magma") +  # Set color scale to light colors using Viridis
facet_wrap(~year, scales = "free", ncol = 2) +           # Wrap facets by year, adjust scales, and arrange in two columns
labs(x = "Term", y = "tf-idf",           # Add labels to axes
title = "Top 20 Characteristic Terms from Trump's Tweets for Each Year") + # Add title
theme(text = element_text(size = 6),      # Set text size
axis.title = element_text(face = "bold", size = 20),panel.spacing = unit(10, "pt"),  # Customize axis title appearance and panel spacing
plot.title = element_text(face = "bold", size = 12)  )  # Customize plot title appearance
# Filter the data to include only tweets from 2016-2020
filtered_data <- tidy %>%
filter(year >= 2016 & year <= 2020)
# Create a document-term matrix
dtm <- filtered_data %>%
count(id, word) %>%
cast_sparse(id, word, n)
# Display dimensions of the document-term matrix
dim(dtm)
# Extract the tweet IDs to get the target variable (# of retweets)
ids <- tibble(id=rownames(dtm))
ids <- left_join(ids, trump_data)
# Define the target variable
retweets <- ids$retweets
# Set seed for reproducibility
set.seed(2020)
# Fit Lasso and Elastic-Net regularized generalized linear models
fit_cv <- cv.glmnet(dtm, retweets)
# Visualize the cross-validation results
plot(fit_cv)
# Display cross-validation results
fit_cv
# Extract the selected value of lambda
selected_lambda <- fit_cv$lambda.min
cat("Selected value of lambda:", selected_lambda, "\n")
# Get the number of non-zero coefficients
coefficients <- coef(fit_cv, s = selected_lambda)
non_zero_coefficients <- sum(coefficients != 0)
cat("Number of non-zero coefficients:", non_zero_coefficients, "\n")
# Extract coefficients from the best model
coefficients <- coef(fit_cv, s = "lambda.min")
# Convert coefficients to a tibble
coefficients <- tibble(word = rownames(coefficients), coef = as.numeric(coefficients))
# Select the top 15 coefficients
top_15_terms <- coefficients %>%
top_n(15)
# Plot the top 15 terms with their coefficients
library(ggplot2)
ggplot(top_15_terms, aes(x = reorder(word, coef), y = coef/10)) + # Create ggplot with top 15 terms and their coefficients
geom_col(fill = "orange", width = 0.5) + # Add column plot with orange color
coord_flip() + # Flip the coordinates
labs(x = "Term", y = "Regression Coefficient", # Add labels to axes
title = "Top 15 Terms with Regression Coefficients") + # Add title
theme_minimal() + # Apply minimal theme
theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Rotate x-axis labels
trump_lower <- trump_data %>%
filter(year(date) >= 2016) %>%
mutate(text=str_to_lower(text))
trump_lower %>%
filter(str_detect(text, "#fnn")) %>%
select(date, text, retweets)
trump_lower %>%
filter(str_detect(text, "insult")) %>%
select(date, text, retweets)
# Filter the dataframe to include only tweets containing the word "starved"
trump_lower %>%
filter(str_detect(text, "starved")) %>%
# Select relevant columns for analysis
select(date, text, retweets)
# Filter tweets containing the pattern "weli" using the lowercased text
trump_lower %>%
filter(str_detect(text, "weli")) %>%
# Select relevant columns for display
select(date, text, retweets)
# Filter the trump_lower dataframe to include only tweets containing the word "quarantine"
trump_lower %>%
filter(str_detect(text, "quarantine")) %>%
select(date, text, retweets)
# Filter tweets containing the term "covfefe"
trump_lower %>%
filter(str_detect(text, "covfefe")) %>%
# Select specific columns for analysis
select(date, text, retweets)
# Filter the 'trump_lower' dataset to include only rows where the text contains the pattern "way[:punct:]+and"
trump_lower %>%
filter(str_detect(text, "way[:punct:]+and")) %>%
# Select specific columns for output
select(date, text, retweets)
# Filter tweets containing the pattern "amazing[:punct:]+tremendous"
trump_lower %>%
filter(str_detect(text, "amazing[:punct:]+tremendous")) %>%
# Select columns for date, text, and retweets
select(date, text, retweets)
